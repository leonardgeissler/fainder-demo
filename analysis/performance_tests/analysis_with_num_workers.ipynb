{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set up logging and CSV export directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = Path(\"figures/analysis/logs\")\n",
    "csv_dir = Path(\"figures/analysis/csv_exports\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = log_dir / f\"analysis_results_{timestamp}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Starting performance analysis - Log file: {log_file}\")\n",
    "logger.info(f\"CSV exports will be saved to: {csv_dir}\")\n",
    "\n",
    "# Get list of all CSV files in the directory\n",
    "list_of_commits = list(Path(\"../../logs/performance/analysis\").glob(\"*/\"))\n",
    "list_of_commits.sort(key=os.path.getmtime, reverse=True)\n",
    "list_of_commits = list_of_commits[:1]\n",
    "# Initialize an empty DataFrame to store combined results\n",
    "df_latest = pd.DataFrame()\n",
    "\n",
    "list_of_files = list(Path(list_of_commits[0] / \"all\").glob(\"*.csv\"))\n",
    "logger.info(f\"Processing {len(list_of_files)} CSV files from {list_of_commits[0]}\")\n",
    "\n",
    "# For each file in this commit\n",
    "for file_path in list_of_files:\n",
    "    # Read the CSV file\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Append to the main DataFrame\n",
    "    df_latest = pd.concat([df_latest, temp_df], ignore_index=True)\n",
    "\n",
    "# Adjust num_workers: subtract 1 if not 0\n",
    "df_latest[\"fainder_max_workers\"] = df_latest[\"fainder_max_workers\"].apply(\n",
    "    lambda x: x - 1 if x != 0 else x\n",
    ")\n",
    "\n",
    "# format: \"timestamp\", \"category\", \"test_name\", \"query\", \"scenario\",\n",
    "#          \"execution_time\", \"results_consistent\", \"fainder_mode\",\n",
    "#          \"num_results\", \"ids\", \"num_terms\", \"id_str\", \"write_groups_used\",\n",
    "#          \"write_groups_actually_used\", \"fainder_parallel\", \"fainder_max_workers\",\n",
    "#          \"fainder_contiguous_chunks\", \"optimizer_cost_sorting\",\n",
    "#          \"optimizer_keyword_merging\", \"optimizer_split_up_junctions\"\n",
    "print(df_latest)\n",
    "\n",
    "# First check the unique category values to see what we're working with\n",
    "print(\"Unique categories:\", df_latest[\"category\"].unique())\n",
    "logger.info(f\"Loaded {len(df_latest)} total records\")\n",
    "logger.info(f\"Unique categories: {list(df_latest['category'].unique())}\")\n",
    "logger.info(f\"Unique fainder modes: {list(df_latest['fainder_mode'].unique())}\")\n",
    "logger.info(f\"Unique scenarios: {list(df_latest['scenario'].unique())}\")\n",
    "logger.info(f\"Worker counts: {sorted(df_latest['fainder_max_workers'].unique())}\")\n",
    "\n",
    "# Strip whitespace and normalize the category column\n",
    "df_latest[\"category\"] = df_latest[\"category\"].str.strip()\n",
    "\n",
    "# Save processed data to CSV\n",
    "processed_data_file = csv_dir / f\"processed_data_{timestamp}.csv\"\n",
    "df_latest.to_csv(processed_data_file, index=False)\n",
    "logger.info(f\"Saved processed data to: {processed_data_file}\")\n",
    "\n",
    "os.makedirs(\"figures/analysis\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame by category with stripped values\n",
    "base_keyword_queries = df_latest[\n",
    "    df_latest[\"category\"] == \"base_keyword_queries\"\n",
    "].reset_index(drop=True)\n",
    "base_keyword_queries_with_multiple_elements = df_latest[\n",
    "    df_latest[\"category\"] == \"base_keyword_queries_with_multiple_elements\"\n",
    "].reset_index(drop=True)\n",
    "base_percentile_queries = df_latest[\n",
    "    df_latest[\"category\"] == \"base_percentile_queries\"\n",
    "].reset_index(drop=True)\n",
    "base_column_name_queries = df_latest[\n",
    "    df_latest[\"category\"] == \"base_column_name_queries\"\n",
    "].reset_index(drop=True)\n",
    "percentile_combinations = df_latest[\n",
    "    df_latest[\"category\"] == \"percentile_combinations\"\n",
    "].reset_index(drop=True)\n",
    "mixed_combinations_with_fixed_structure = df_latest[\n",
    "    df_latest[\"category\"] == \"mixed_combinations_with_fixed_structure\"\n",
    "].reset_index(drop=True)\n",
    "mixed_combinations_with_fixed_structure_extented = df_latest[\n",
    "    df_latest[\"category\"] == \"mixed_combinations_with_fixed_structure_extented\"\n",
    "].reset_index(drop=True)\n",
    "early_exit = df_latest[df_latest[\"category\"] == \"early_exit\"].reset_index(drop=True)\n",
    "# Filter for multiple percentile combinations\n",
    "multiple_percentile_combinations = df_latest[\n",
    "    df_latest[\"category\"] == \"multiple_percentile_combinations\"\n",
    "].reset_index(drop=True)\n",
    "multiple_percentile_combinations_with_kw = df_latest[\n",
    "    df_latest[\"category\"] == \"multiple_percentile_combinations_with_kw\"\n",
    "].reset_index(drop=True)\n",
    "expected_form_not_queries = df_latest[\n",
    "    df_latest[\"category\"] == \"expected_form_not_queries\"\n",
    "].reset_index(drop=True)\n",
    "double_expected_form_queries = df_latest[\n",
    "    df_latest[\"category\"] == \"double_expected_form_queries\"\n",
    "].reset_index(drop=True)\n",
    "middle_exit = df_latest[df_latest[\"category\"] == \"middle_exit\"].reset_index(drop=True)\n",
    "\n",
    "# Print to verify the filtering worked\n",
    "print(\"\\nCounts after fixing whitespace:\")\n",
    "print(\"Base keyword queries:\", len(base_keyword_queries))\n",
    "print(\n",
    "    \"Base keyword queries with multiple elements:\",\n",
    "    len(base_keyword_queries_with_multiple_elements),\n",
    ")\n",
    "print(\"Base percentile queries:\", len(base_percentile_queries))\n",
    "print(\"Percentile combinations:\", len(percentile_combinations))\n",
    "print(\n",
    "    \"Mixed combinations with fixed structure:\",\n",
    "    len(mixed_combinations_with_fixed_structure),\n",
    ")\n",
    "print(\n",
    "    \"Mixed combinations with fixed structure extended:\",\n",
    "    len(mixed_combinations_with_fixed_structure_extented),\n",
    ")\n",
    "print(\"Early exit:\", len(early_exit))\n",
    "print(\"Multiple percentile combinations:\", len(multiple_percentile_combinations))\n",
    "print(\n",
    "    \"Multiple percentile combinations with keyword:\",\n",
    "    len(multiple_percentile_combinations_with_kw),\n",
    ")\n",
    "print(\"Expected form not queries:\", len(expected_form_not_queries))\n",
    "print(\"Double expected form queries:\", len(double_expected_form_queries))\n",
    "print(\"Middle exit:\", len(middle_exit))\n",
    "\n",
    "# Log and save category counts to CSV\n",
    "logger.info(\"=== Category Filtering Results ===\")\n",
    "category_counts = {\n",
    "    \"base_keyword_queries\": len(base_keyword_queries),\n",
    "    \"base_keyword_queries_with_multiple_elements\": len(base_keyword_queries_with_multiple_elements),\n",
    "    \"base_percentile_queries\": len(base_percentile_queries),\n",
    "    \"base_column_name_queries\": len(base_column_name_queries),\n",
    "    \"percentile_combinations\": len(percentile_combinations),\n",
    "    \"mixed_combinations_with_fixed_structure\": len(mixed_combinations_with_fixed_structure),\n",
    "    \"mixed_combinations_with_fixed_structure_extented\": len(mixed_combinations_with_fixed_structure_extented),\n",
    "    \"early_exit\": len(early_exit),\n",
    "    \"multiple_percentile_combinations\": len(multiple_percentile_combinations),\n",
    "    \"multiple_percentile_combinations_with_kw\": len(multiple_percentile_combinations_with_kw),\n",
    "    \"expected_form_not_queries\": len(expected_form_not_queries),\n",
    "    \"double_expected_form_queries\": len(double_expected_form_queries),\n",
    "    \"middle_exit\": len(middle_exit)\n",
    "}\n",
    "\n",
    "for category, count in category_counts.items():\n",
    "    logger.info(f\"{category}: {count} records\")\n",
    "\n",
    "# Save category counts to CSV\n",
    "category_counts_df = pd.DataFrame(list(category_counts.items()), columns=['category', 'count'])\n",
    "category_counts_file = csv_dir / f\"category_counts_{timestamp}.csv\"\n",
    "category_counts_df.to_csv(category_counts_file, index=False)\n",
    "logger.info(f\"Saved category counts to: {category_counts_file}\")\n",
    "\n",
    "# Save individual category datasets to CSV\n",
    "category_datasets = {\n",
    "    \"base_keyword_queries\": base_keyword_queries,\n",
    "    \"base_keyword_queries_with_multiple_elements\": base_keyword_queries_with_multiple_elements,\n",
    "    \"base_percentile_queries\": base_percentile_queries,\n",
    "    \"base_column_name_queries\": base_column_name_queries,\n",
    "    \"percentile_combinations\": percentile_combinations,\n",
    "    \"mixed_combinations_with_fixed_structure\": mixed_combinations_with_fixed_structure,\n",
    "    \"mixed_combinations_with_fixed_structure_extented\": mixed_combinations_with_fixed_structure_extented,\n",
    "    \"early_exit\": early_exit,\n",
    "    \"multiple_percentile_combinations\": multiple_percentile_combinations,\n",
    "    \"multiple_percentile_combinations_with_kw\": multiple_percentile_combinations_with_kw,\n",
    "    \"expected_form_not_queries\": expected_form_not_queries,\n",
    "    \"double_expected_form_queries\": double_expected_form_queries,\n",
    "    \"middle_exit\": middle_exit\n",
    "}\n",
    "\n",
    "for cat_name, cat_data in category_datasets.items():\n",
    "    if not cat_data.empty:\n",
    "        cat_file = csv_dir / f\"{cat_name}_{timestamp}.csv\"\n",
    "        cat_data.to_csv(cat_file, index=False)\n",
    "        logger.info(f\"Saved {cat_name} data to: {cat_file}\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "# Get unique scenarios and colors for plotting\n",
    "scenarios = base_keyword_queries[\"scenario\"].unique()\n",
    "colors = [\"r\", \"g\", \"b\", \"y\", \"c\", \"m\", \"k\"]\n",
    "logger.info(f\"Found {len(scenarios)} unique scenarios: {list(scenarios)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a plot with three lines (one base_keyword_queries and two percentile queries (different fainder modes) per num_workers) x-axis is queryid\n",
    "\n",
    "logger.info(\"=== Base Predicates Analysis ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.title(\"Base Keyword Queries vs Percentile Queries\")\n",
    "plt.xlabel(\"Query ID\")\n",
    "plt.ylabel(\"Execution Time (s)\")\n",
    "fainder_modes = base_keyword_queries[\"fainder_mode\"].unique()\n",
    "\n",
    "# Prepare list to collect summary statistics for CSV export\n",
    "base_predicates_stats = []\n",
    "\n",
    "# Plot base keyword queries\n",
    "base_keyword_queries_grouped = (\n",
    "    base_keyword_queries.groupby(\"query\")[\"execution_time\"].mean().reset_index()\n",
    ")\n",
    "plt.plot(\n",
    "    base_keyword_queries_grouped.index,\n",
    "    base_keyword_queries_grouped[\"execution_time\"],\n",
    "    label=\"Base Keyword Queries\",\n",
    "    color=colors[0],\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "# Log and collect base keyword queries stats\n",
    "base_kw_stats = base_keyword_queries_grouped[\"execution_time\"].describe()\n",
    "logger.info(f\"Base Keyword Queries - Mean: {base_kw_stats['mean']:.4f}s, \"\n",
    "           f\"Median: {base_kw_stats['50%']:.4f}s, \"\n",
    "           f\"Min: {base_kw_stats['min']:.4f}s, \"\n",
    "           f\"Max: {base_kw_stats['max']:.4f}s\")\n",
    "\n",
    "base_predicates_stats.append({\n",
    "    'query_type': 'Base Keyword Queries',\n",
    "    'fainder_mode': 'N/A',\n",
    "    'workers': 'N/A',\n",
    "    'mean': base_kw_stats['mean'],\n",
    "    'median': base_kw_stats['50%'],\n",
    "    'min': base_kw_stats['min'],\n",
    "    'max': base_kw_stats['max'],\n",
    "    'std': base_kw_stats['std'],\n",
    "    'count': base_kw_stats['count']\n",
    "})\n",
    "\n",
    "# Plot column name queries\n",
    "base_column_name_queries_grouped = (\n",
    "    base_column_name_queries.groupby(\"query\")[\"execution_time\"].mean().reset_index()\n",
    ")\n",
    "plt.plot(\n",
    "    base_column_name_queries_grouped.index,\n",
    "    base_column_name_queries_grouped[\"execution_time\"],\n",
    "    label=\"Base Column Name Queries\",\n",
    "    color=colors[1],\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "# Log and collect column name queries stats\n",
    "if not base_column_name_queries_grouped.empty:\n",
    "    col_name_stats = base_column_name_queries_grouped[\"execution_time\"].describe()\n",
    "    logger.info(f\"Base Column Name Queries - Mean: {col_name_stats['mean']:.4f}s, \"\n",
    "               f\"Median: {col_name_stats['50%']:.4f}s, \"\n",
    "               f\"Min: {col_name_stats['min']:.4f}s, \"\n",
    "               f\"Max: {col_name_stats['max']:.4f}s\")\n",
    "    \n",
    "    base_predicates_stats.append({\n",
    "        'query_type': 'Base Column Name Queries',\n",
    "        'fainder_mode': 'N/A',\n",
    "        'workers': 'N/A',\n",
    "        'mean': col_name_stats['mean'],\n",
    "        'median': col_name_stats['50%'],\n",
    "        'min': col_name_stats['min'],\n",
    "        'max': col_name_stats['max'],\n",
    "        'std': col_name_stats['std'],\n",
    "        'count': col_name_stats['count']\n",
    "    })\n",
    "\n",
    "i = 2\n",
    "# Plot base percentile queries not exact mode\n",
    "fainder_modes_without_exact = [mode for mode in fainder_modes if mode != \"exact\"]\n",
    "\n",
    "for fainder_mode in fainder_modes_without_exact:\n",
    "    mode_queries = base_percentile_queries[\n",
    "        base_percentile_queries[\"fainder_mode\"] == fainder_mode\n",
    "    ].reset_index(drop=True)\n",
    "    mode_queries_grouped = mode_queries.groupby(\"query\")[\"execution_time\"].mean().reset_index()\n",
    "    plt.plot(\n",
    "        mode_queries_grouped.index,\n",
    "        mode_queries_grouped[\"execution_time\"],\n",
    "        label=f\"Base Percentile Queries ({fainder_mode})\",\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    \n",
    "    # Log and collect percentile queries stats for this mode\n",
    "    if not mode_queries_grouped.empty:\n",
    "        mode_stats = mode_queries_grouped[\"execution_time\"].describe()\n",
    "        logger.info(f\"Base Percentile Queries ({fainder_mode}) - Mean: {mode_stats['mean']:.4f}s, \"\n",
    "                   f\"Median: {mode_stats['50%']:.4f}s, \"\n",
    "                   f\"Min: {mode_stats['min']:.4f}s, \"\n",
    "                   f\"Max: {mode_stats['max']:.4f}s\")\n",
    "        \n",
    "        base_predicates_stats.append({\n",
    "            'query_type': 'Base Percentile Queries',\n",
    "            'fainder_mode': fainder_mode,\n",
    "            'workers': 'N/A',\n",
    "            'mean': mode_stats['mean'],\n",
    "            'median': mode_stats['50%'],\n",
    "            'min': mode_stats['min'],\n",
    "            'max': mode_stats['max'],\n",
    "            'std': mode_stats['std'],\n",
    "            'count': mode_stats['count']\n",
    "        })\n",
    "    i += 1\n",
    "\n",
    "# Plot base percentile queries [exact]\n",
    "num_workers = base_percentile_queries[\"fainder_max_workers\"].unique()\n",
    "\n",
    "fainder_mode_exact = \"exact\"\n",
    "for num_worker in num_workers:\n",
    "    percentile_queries = base_percentile_queries[\n",
    "        base_percentile_queries[\"fainder_max_workers\"] == num_worker\n",
    "    ].reset_index(drop=True)\n",
    "    mode_queries = percentile_queries[\n",
    "        percentile_queries[\"fainder_mode\"] == fainder_mode_exact\n",
    "    ].reset_index(drop=True)\n",
    "    mode_queries_grouped = mode_queries.groupby(\"query\")[\"execution_time\"].mean().reset_index()\n",
    "    # Adjust worker display: show actual worker count\n",
    "    display_workers = num_worker + 1 if num_worker != 0 else 0\n",
    "    plt.plot(\n",
    "        mode_queries_grouped.index,\n",
    "        mode_queries_grouped[\"execution_time\"],\n",
    "        label=f\"Percentile Queries ({fainder_mode_exact}, {display_workers} workers)\",\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    \n",
    "    # Log and collect exact mode stats for this worker count\n",
    "    if not mode_queries_grouped.empty:\n",
    "        worker_stats = mode_queries_grouped[\"execution_time\"].describe()\n",
    "        logger.info(f\"Percentile Queries (exact, {display_workers} workers) - Mean: {worker_stats['mean']:.4f}s, \"\n",
    "                   f\"Median: {worker_stats['50%']:.4f}s, \"\n",
    "                   f\"Min: {worker_stats['min']:.4f}s, \"\n",
    "                   f\"Max: {worker_stats['max']:.4f}s\")\n",
    "        \n",
    "        base_predicates_stats.append({\n",
    "            'query_type': 'Percentile Queries',\n",
    "            'fainder_mode': fainder_mode_exact,\n",
    "            'workers': display_workers,\n",
    "            'mean': worker_stats['mean'],\n",
    "            'median': worker_stats['50%'],\n",
    "            'min': worker_stats['min'],\n",
    "            'max': worker_stats['max'],\n",
    "            'std': worker_stats['std'],\n",
    "            'count': worker_stats['count']\n",
    "        })\n",
    "    i += 1\n",
    "\n",
    "# Save base predicates statistics to CSV\n",
    "base_predicates_stats_df = pd.DataFrame(base_predicates_stats)\n",
    "base_predicates_stats_file = csv_dir / f\"base_predicates_stats_{timestamp}.csv\"\n",
    "base_predicates_stats_df.to_csv(base_predicates_stats_file, index=False)\n",
    "logger.info(f\"Saved base predicates statistics to: {base_predicates_stats_file}\")\n",
    "\n",
    "plt.xticks()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/analysis/base_predicates.png\")\n",
    "logger.info(\"Saved base predicates plot to figures/analysis/base_predicates.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "categories = [\"NOT_Combinations\"]\n",
    "fainder_modes = df_latest[\"fainder_mode\"].unique()\n",
    "\n",
    "logger.info(\"=== NOT Combinations Heatmap Analysis ===\")\n",
    "\n",
    "# List to collect all heatmap data for CSV export\n",
    "not_combinations_data = []\n",
    "\n",
    "for category in categories:\n",
    "    df_category = df_latest[df_latest[\"category\"] == category]\n",
    "    logger.info(f\"Analyzing category: {category} with {len(df_category)} records\")\n",
    "\n",
    "    for fainder_mode in fainder_modes:\n",
    "        df_fainder_mode = df_category[df_category[\"fainder_mode\"] == fainder_mode]\n",
    "        logger.info(f\"  Fainder mode: {fainder_mode} with {len(df_fainder_mode)} records\")\n",
    "\n",
    "        unique_ids = sorted(\n",
    "            df_fainder_mode[\"ids\"].unique(), key=lambda x: (len(str(x)), str(x))\n",
    "        )\n",
    "        id_groups = {\n",
    "            \"single\": [id_ for id_ in unique_ids if len(str(id_).split(\"-\")) == 1],\n",
    "            \"double\": [id_ for id_ in unique_ids if len(str(id_).split(\"-\")) == 2],\n",
    "            \"triple\": [\n",
    "                id_\n",
    "                for id_ in unique_ids\n",
    "                if len(str(id_).split(\"-\")) == 3 or len(str(id_).split(\"-\")) == 4\n",
    "            ],\n",
    "            \"all\": unique_ids,\n",
    "        }\n",
    "\n",
    "        # Log ID group information\n",
    "        for group_name, group_ids in id_groups.items():\n",
    "            logger.info(f\"    {group_name} group: {len(group_ids)} IDs\")\n",
    "\n",
    "        # Create a heatmap for each group\n",
    "        for group_name, group_ids in id_groups.items():\n",
    "            if not group_ids:  # Skip if no IDs in this group\n",
    "                continue\n",
    "\n",
    "            # Get unique workers and scenarios\n",
    "            unique_workers = sorted(df_fainder_mode[\"fainder_max_workers\"].unique())\n",
    "            unique_scenarios = df_fainder_mode[\"scenario\"].unique()\n",
    "\n",
    "            # Create a pivot table for the heatmap\n",
    "            # Prepare data for heatmap: rows=worker-scenario combinations, columns=IDs\n",
    "            heatmap_data = []\n",
    "            row_labels = []\n",
    "\n",
    "            for worker in unique_workers:\n",
    "                worker_data = df_fainder_mode[\n",
    "                    df_fainder_mode[\"fainder_max_workers\"] == worker\n",
    "                ]\n",
    "\n",
    "                for scenario in unique_scenarios:\n",
    "                    scenario_data = worker_data[worker_data[\"scenario\"] == scenario]\n",
    "                    if not scenario_data.empty:\n",
    "                        # Filter for current group's IDs and calculate mean execution time\n",
    "                        grouped_data = (\n",
    "                            scenario_data[scenario_data[\"ids\"].isin(group_ids)]\n",
    "                            .groupby(\"ids\")[\"execution_time\"]\n",
    "                            .mean()\n",
    "                        )\n",
    "\n",
    "                        # Create row for this worker-scenario combination\n",
    "                        row_values = [\n",
    "                            grouped_data.get(id_, np.nan) for id_ in group_ids\n",
    "                        ]\n",
    "                        heatmap_data.append(row_values)\n",
    "                        row_labels.append(f\"{worker}w-{scenario}\")\n",
    "\n",
    "            # Convert to numpy array\n",
    "            heatmap_array = np.array(heatmap_data)\n",
    "\n",
    "            # Save heatmap data to CSV\n",
    "            heatmap_df = pd.DataFrame(heatmap_array, index=row_labels, columns=group_ids)\n",
    "            heatmap_csv_file = csv_dir / f\"heatmap_{category}_{fainder_mode}_{group_name}_{timestamp}.csv\"\n",
    "            heatmap_df.to_csv(heatmap_csv_file)\n",
    "            logger.info(f\"    Saved heatmap data to: {heatmap_csv_file}\")\n",
    "\n",
    "            # Log heatmap statistics and collect for summary\n",
    "            non_nan_values = heatmap_array[~np.isnan(heatmap_array)]\n",
    "            if len(non_nan_values) > 0:\n",
    "                heatmap_stats = {\n",
    "                    'category': category,\n",
    "                    'fainder_mode': fainder_mode,\n",
    "                    'group': group_name,\n",
    "                    'mean': np.mean(non_nan_values),\n",
    "                    'min': np.min(non_nan_values),\n",
    "                    'max': np.max(non_nan_values),\n",
    "                    'std': np.std(non_nan_values),\n",
    "                    'valid_points': len(non_nan_values),\n",
    "                    'total_points': heatmap_array.size\n",
    "                }\n",
    "                not_combinations_data.append(heatmap_stats)\n",
    "                \n",
    "                logger.info(f\"    {group_name} heatmap stats - Mean: {heatmap_stats['mean']:.4f}s, \"\n",
    "                           f\"Min: {heatmap_stats['min']:.4f}s, \"\n",
    "                           f\"Max: {heatmap_stats['max']:.4f}s, \"\n",
    "                           f\"Valid data points: {heatmap_stats['valid_points']}\")\n",
    "\n",
    "            # Create the heatmap\n",
    "            plt.figure(\n",
    "                figsize=(max(8, len(group_ids) * 1.2), max(6, len(row_labels) * 0.4))\n",
    "            )\n",
    "\n",
    "            sns.heatmap(\n",
    "                heatmap_array,\n",
    "                xticklabels=group_ids,\n",
    "                yticklabels=row_labels,\n",
    "                annot=True,\n",
    "                fmt=\".3f\",\n",
    "                cmap=\"viridis\",\n",
    "                cbar_kws={\"label\": \"Execution Time (s)\"},\n",
    "            )\n",
    "            #plt.title(f\"{group_name.title()} IDs - {category} - {fainder_mode}\")\n",
    "\n",
    "            plt.xlabel(\"ID\")\n",
    "            plt.ylabel(\"Worker-Scenario Combination\")\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            filename = f\"figures/analysis/{category}_{fainder_mode}_{group_name}_heatmap.png\"\n",
    "            plt.savefig(\n",
    "                filename,\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )\n",
    "            logger.info(f\"    Saved heatmap: {filename}\")\n",
    "            plt.show()\n",
    "\n",
    "# Save NOT combinations summary statistics to CSV\n",
    "if not_combinations_data:\n",
    "    not_combinations_stats_df = pd.DataFrame(not_combinations_data)\n",
    "    not_combinations_stats_file = csv_dir / f\"not_combinations_stats_{timestamp}.csv\"\n",
    "    not_combinations_stats_df.to_csv(not_combinations_stats_file, index=False)\n",
    "    logger.info(f\"Saved NOT combinations statistics to: {not_combinations_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare execution time (as heatmap) against scenario and num_workers for one plot per fainder mode\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "logger.info(\"=== Execution Time Heatmaps by Fainder Mode ===\")\n",
    "\n",
    "# List to collect all heatmap statistics for CSV export\n",
    "execution_time_heatmap_stats = []\n",
    "\n",
    "for fainder_mode in df_latest[\"fainder_mode\"].unique():\n",
    "    df_fainder_mode = df_latest[df_latest[\"fainder_mode\"] == fainder_mode]\n",
    "    logger.info(f\"Analyzing fainder mode: {fainder_mode} with {len(df_fainder_mode)} records\")\n",
    "\n",
    "    # Also create separate heatmaps for the min, mean and max values\n",
    "    for stat, title_suffix in [\n",
    "        (\"min\", \"Minimum\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"max\", \"Maximum\"),\n",
    "    ]:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"{title_suffix} Execution Time Heatmap - {fainder_mode}\")\n",
    "\n",
    "        pivot_stat = df_fainder_mode.pivot_table(\n",
    "            index=\"fainder_max_workers\",\n",
    "            columns=\"scenario\",\n",
    "            values=\"execution_time\",\n",
    "            aggfunc=stat,\n",
    "        ).sort_index()\n",
    "\n",
    "        # Save pivot table to CSV\n",
    "        pivot_csv_file = csv_dir / f\"heatmap_{fainder_mode}_{stat}_{timestamp}.csv\"\n",
    "        pivot_stat.to_csv(pivot_csv_file)\n",
    "        logger.info(f\"  Saved {stat} pivot table to: {pivot_csv_file}\")\n",
    "\n",
    "        # Log statistics for this pivot table\n",
    "        valid_values = pivot_stat.values[~np.isnan(pivot_stat.values)]\n",
    "        if len(valid_values) > 0:\n",
    "            if stat == 'mean':\n",
    "                overall_stat_value = np.mean(valid_values)\n",
    "            elif stat == 'min':\n",
    "                overall_stat_value = np.min(valid_values)\n",
    "            else:  # max\n",
    "                overall_stat_value = np.max(valid_values)\n",
    "            \n",
    "            logger.info(f\"  {title_suffix} heatmap - Overall {stat}: {overall_stat_value:.4f}s\")\n",
    "            \n",
    "            # Collect overall statistics\n",
    "            heatmap_overall_stats = {\n",
    "                'fainder_mode': fainder_mode,\n",
    "                'statistic': stat,\n",
    "                'overall_value': overall_stat_value,\n",
    "                'data_points': len(valid_values),\n",
    "                'mean_of_values': np.mean(valid_values),\n",
    "                'std_of_values': np.std(valid_values)\n",
    "            }\n",
    "            execution_time_heatmap_stats.append(heatmap_overall_stats)\n",
    "            \n",
    "            # Log per-worker statistics\n",
    "            for worker in pivot_stat.index:\n",
    "                worker_values = pivot_stat.loc[worker].dropna()\n",
    "                if len(worker_values) > 0:\n",
    "                    if stat == 'mean':\n",
    "                        worker_stat = worker_values.mean()\n",
    "                    elif stat == 'min':\n",
    "                        worker_stat = worker_values.min()\n",
    "                    else:  # max\n",
    "                        worker_stat = worker_values.max()\n",
    "                    logger.info(f\"    Worker {worker}: {worker_stat:.4f}s\")\n",
    "\n",
    "        sns.heatmap(\n",
    "            pivot_stat,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"label\": \"Execution Time (s)\"},\n",
    "        )\n",
    "\n",
    "        plt.ylabel(\"Number of Workers\")\n",
    "        plt.xlabel(\"Scenario\")\n",
    "        plt.tight_layout()\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        filename = f\"figures/analysis/heatmap_{fainder_mode}_{stat}.png\"\n",
    "        plt.savefig(filename)\n",
    "        logger.info(f\"  Saved {stat} heatmap: {filename}\")\n",
    "\n",
    "    # Create a combined heatmap for the min, mean and max values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #plt.title(f\"Combined Execution Time Heatmap - {fainder_mode}\")\n",
    "    pivot_combined = df_fainder_mode.pivot_table(\n",
    "        index=\"fainder_max_workers\",\n",
    "        columns=\"scenario\",\n",
    "        values=\"execution_time\",\n",
    "        aggfunc=[\"min\", \"mean\", \"max\"],\n",
    "    )\n",
    "    pivot_combined.columns = [f\"{stat} {col}\" for stat, col in pivot_combined.columns]\n",
    "    \n",
    "    # Save combined pivot table to CSV\n",
    "    combined_csv_file = csv_dir / f\"heatmap_combined_{fainder_mode}_{timestamp}.csv\"\n",
    "    pivot_combined.to_csv(combined_csv_file)\n",
    "    logger.info(f\"  Saved combined pivot table to: {combined_csv_file}\")\n",
    "    \n",
    "    # Log combined statistics\n",
    "    logger.info(f\"  Combined heatmap statistics:\")\n",
    "    for stat in [\"min\", \"mean\", \"max\"]:\n",
    "        stat_columns = [col for col in pivot_combined.columns if col.startswith(stat)]\n",
    "        if stat_columns:\n",
    "            stat_values = pivot_combined[stat_columns].values.flatten()\n",
    "            stat_values = stat_values[~np.isnan(stat_values)]\n",
    "            if len(stat_values) > 0:\n",
    "                logger.info(f\"    {stat.title()}: mean={np.mean(stat_values):.4f}s, \"\n",
    "                           f\"min={np.min(stat_values):.4f}s, max={np.max(stat_values):.4f}s\")\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_combined,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"label\": \"Execution Time (s)\"},\n",
    "    )\n",
    "    plt.ylabel(\"Number of Workers\")\n",
    "    plt.xlabel(\"Scenario\")\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    \n",
    "    filename = f\"figures/analysis/heatmap_combined_{fainder_mode}.png\"\n",
    "    plt.savefig(filename)\n",
    "    logger.info(f\"  Saved combined heatmap: {filename}\")\n",
    "\n",
    "# Save execution time heatmap statistics to CSV\n",
    "if execution_time_heatmap_stats:\n",
    "    execution_time_stats_df = pd.DataFrame(execution_time_heatmap_stats)\n",
    "    execution_time_stats_file = csv_dir / f\"execution_time_heatmap_stats_{timestamp}.csv\"\n",
    "    execution_time_stats_df.to_csv(execution_time_stats_file, index=False)\n",
    "    logger.info(f\"Saved execution time heatmap statistics to: {execution_time_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare execution time (y-axis min, max, mean) against scenario and num_workers (x-axis) for one plot per fainder mode per category one plot\n",
    "categories = df_latest[\"category\"].unique()\n",
    "fainder_modes = [\"exact\"]\n",
    "\n",
    "logger.info(\"=== Execution Time Analysis by Workers and Scenario ===\")\n",
    "\n",
    "# List to collect all worker-scenario statistics for CSV export\n",
    "worker_scenario_stats = []\n",
    "\n",
    "for category in categories:\n",
    "    df_category = df_latest[df_latest[\"category\"] == category]\n",
    "    logger.info(f\"Analyzing category: {category} with {len(df_category)} records\")\n",
    "\n",
    "    for fainder_mode in fainder_modes:\n",
    "        df_fainder_mode = df_category[df_category[\"fainder_mode\"] == fainder_mode]\n",
    "        logger.info(f\"  Fainder mode: {fainder_mode} with {len(df_fainder_mode)} records\")\n",
    "\n",
    "        if df_fainder_mode.empty:\n",
    "            logger.info(f\"    No data for {category} - {fainder_mode}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Group first by workers then by scenario\n",
    "        grouped_data = df_fainder_mode.groupby([\"fainder_max_workers\", \"scenario\"])\n",
    "\n",
    "        # Collect detailed statistics for each worker-scenario combination\n",
    "        category_scenario_data = []\n",
    "        \n",
    "        for (workers, scenario), group in grouped_data:\n",
    "            execution_times = group[\"execution_time\"]\n",
    "\n",
    "            # Calculate statistics\n",
    "            min_time = execution_times.min()\n",
    "            max_time = execution_times.max()\n",
    "            mean_time = execution_times.mean()\n",
    "            std_time = execution_times.std()\n",
    "            count = len(execution_times)\n",
    "            \n",
    "            stats_record = {\n",
    "                'category': category,\n",
    "                'fainder_mode': fainder_mode,\n",
    "                'workers': workers,\n",
    "                'scenario': scenario,\n",
    "                'mean': mean_time,\n",
    "                'min': min_time,\n",
    "                'max': max_time,\n",
    "                'std': std_time,\n",
    "                'count': count,\n",
    "                'median': execution_times.median(),\n",
    "                'q25': execution_times.quantile(0.25),\n",
    "                'q75': execution_times.quantile(0.75)\n",
    "            }\n",
    "            \n",
    "            category_scenario_data.append(stats_record)\n",
    "            worker_scenario_stats.append(stats_record)\n",
    "            \n",
    "            logger.info(f\"      {workers} workers, {scenario}: \"\n",
    "                       f\"mean={mean_time:.4f}s, min={min_time:.4f}s, max={max_time:.4f}s \"\n",
    "                       f\"(n={count})\")\n",
    "\n",
    "        # Save category-specific worker-scenario data to CSV\n",
    "        if category_scenario_data:\n",
    "            category_scenario_df = pd.DataFrame(category_scenario_data)\n",
    "            category_scenario_file = csv_dir / f\"worker_scenario_{category}_{fainder_mode}_{timestamp}.csv\"\n",
    "            category_scenario_df.to_csv(category_scenario_file, index=False)\n",
    "            logger.info(f\"    Saved {category} worker-scenario data to: {category_scenario_file}\")\n",
    "\n",
    "        # Create two figures - one with linear scale, one with log scale\n",
    "        for scale_type in [\"linear\", \"log\"]:\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            #plt.title(\n",
    "            #    f\"Execution Time by Workers and Scenario - {category} - {fainder_mode} ({scale_type} scale)\"\n",
    "            #)\n",
    "            plt.xlabel(\"Scenario\")\n",
    "            plt.ylabel(\"Execution Time (s)\")\n",
    "\n",
    "            # Set log scale if needed\n",
    "            if scale_type == \"log\":\n",
    "                plt.yscale(\"log\")\n",
    "                # Force y-axis to show actual numbers instead of scientific notation\n",
    "                plt.gca().yaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "                plt.gca().yaxis.set_minor_formatter(plt.ScalarFormatter())\n",
    "\n",
    "            # Get unique workers and scenarios\n",
    "            unique_workers = sorted(df_fainder_mode[\"fainder_max_workers\"].unique())\n",
    "            unique_scenarios = df_fainder_mode[\"scenario\"].unique()\n",
    "\n",
    "            # Set up positions for bar groups with smaller separation between same workers\n",
    "            groups = list(grouped_data.groups.keys())\n",
    "\n",
    "            # Create a dictionary to map (worker, scenario) to x position\n",
    "            x_pos_map = {}\n",
    "            current_x = 0\n",
    "\n",
    "            for worker in unique_workers:\n",
    "                # Create positions for this worker's scenarios with small gaps between them\n",
    "                scenario_positions = [\n",
    "                    current_x + i * 0.3 for i in range(len(unique_scenarios))\n",
    "                ]\n",
    "                for i, scenario in enumerate(unique_scenarios):\n",
    "                    x_pos_map[(worker, scenario)] = scenario_positions[i]\n",
    "\n",
    "                # Add larger gap between different worker groups\n",
    "                current_x = scenario_positions[-1] + 1\n",
    "\n",
    "            # Get x positions for each group\n",
    "            x_positions = [x_pos_map[(worker, scenario)] for worker, scenario in groups]\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = [f\"{workers} - {scenario}\" for workers, scenario in groups]\n",
    "\n",
    "            # Collect data for plotting\n",
    "            min_times = []\n",
    "            max_times = []\n",
    "            mean_times = []\n",
    "\n",
    "            logger.info(f\"    {scale_type} scale analysis:\")\n",
    "            for (workers, scenario), group in grouped_data:\n",
    "                execution_times = group[\"execution_time\"]\n",
    "\n",
    "                # Calculate statistics\n",
    "                min_time = execution_times.min()\n",
    "                max_time = execution_times.max()\n",
    "                mean_time = execution_times.mean()\n",
    "                \n",
    "                min_times.append(min_time)\n",
    "                max_times.append(max_time)\n",
    "                mean_times.append(mean_time)\n",
    "\n",
    "            # Create bars for mean values\n",
    "            plt.bar(x_positions, mean_times, width=0.25, alpha=0.7, label=\"Mean\")\n",
    "\n",
    "            # Add markers for min and max\n",
    "            plt.scatter(\n",
    "                x_positions, min_times, marker=\"v\", color=\"blue\", s=50, label=\"Min\"\n",
    "            )\n",
    "            plt.scatter(\n",
    "                x_positions, max_times, marker=\"^\", color=\"red\", s=50, label=\"Max\"\n",
    "            )\n",
    "\n",
    "            # Add vertical lines to separate worker groups\n",
    "            current_line_x = -0.5\n",
    "            for worker in unique_workers[:-1]:  # Don't add line after last worker\n",
    "                current_line_x = x_pos_map[(worker, unique_scenarios[-1])] + 0.5\n",
    "                plt.axvline(x=current_line_x, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "            # Add worker labels centered for each group\n",
    "            for worker in unique_workers:\n",
    "                first_x = x_pos_map[(worker, unique_scenarios[0])]\n",
    "                last_x = x_pos_map[(worker, unique_scenarios[-1])]\n",
    "\n",
    "                # worker underneath the top of the y-axis\n",
    "                if scale_type == \"log\":\n",
    "                    y_bottom, y_top = plt.ylim()\n",
    "                    y_pos = np.exp(\n",
    "                        np.log(y_top) - (np.log(y_top) - np.log(y_bottom)) * 0.05\n",
    "                    )\n",
    "                else:\n",
    "                    y_pos = plt.ylim()[1] * 0.95\n",
    "\n",
    "                plt.text(\n",
    "                    (first_x + last_x) / 2,\n",
    "                    y_pos,\n",
    "                    f\"num_workers: {worker}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=10,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "            # Set the x-tick labels to scenario names (not workers)\n",
    "            plt.xticks(\n",
    "                x_positions,\n",
    "                [scenario for _, scenario in groups],\n",
    "                rotation=45,\n",
    "                ha=\"right\",\n",
    "            )\n",
    "\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if scale_type == \"linear\":\n",
    "                filename = f\"figures/analysis/{category}_{fainder_mode}_by_workers_scenario.png\"\n",
    "            else:\n",
    "                filename = f\"figures/analysis/{category}_{fainder_mode}_by_workers_scenario_{scale_type}.png\"\n",
    "            \n",
    "            plt.savefig(filename)\n",
    "            logger.info(f\"    Saved {scale_type} scale plot: {filename}\")\n",
    "\n",
    "# Save all worker-scenario statistics to CSV\n",
    "if worker_scenario_stats:\n",
    "    worker_scenario_stats_df = pd.DataFrame(worker_scenario_stats)\n",
    "    worker_scenario_stats_file = csv_dir / f\"all_worker_scenario_stats_{timestamp}.csv\"\n",
    "    worker_scenario_stats_df.to_csv(worker_scenario_stats_file, index=False)\n",
    "    logger.info(f\"Saved all worker-scenario statistics to: {worker_scenario_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_categories = {\n",
    "    \"Only_Percentile_Combinations\": [\n",
    "        \"Percentile_Combinations\",\n",
    "        \"Multiple_Percentile_Combinations\",\n",
    "    ],\n",
    "    \"Keyword_and_pp\": [\n",
    "        \"Expected_Form\",\n",
    "        \"Expected_Form_Extended\",\n",
    "        \"Multiple_percentile_combinations_with_Keyword\",\n",
    "        \"Double_expected_Form\",\n",
    "    ],\n",
    "    \"Early_Empty_Results\": [\"Early_exit_Results\", \"Middle_exit_Results\"],\n",
    "}\n",
    "\n",
    "ordered_scenarios = [\"sequential\", \"perfiltering\", \"threaded\", \"threaded_prefiltering\"]\n",
    "# heatmap per group_of_categories and per fainder_mode\n",
    "# x-axis: category, y-axis: scenario - fainder_max_workers, values: execution_time\n",
    "\n",
    "category_short_names = {\n",
    "    \"Percentile_Combinations\": \"Percentile Comb.\",\n",
    "    \"Multiple_Percentile_Combinations\": \"Multi Percentile Comb.\",\n",
    "    \"Expected_Form\": \"Expected Form\",\n",
    "    \"Expected_Form_Extended\": \"Expected Form Ext.\",\n",
    "    \"Multiple_percentile_combinations_with_Keyword\": \"Multi Percentile Comb. + KW\",\n",
    "    \"Double_expected_Form\": \"Double Expected Form\",\n",
    "    \"Early_exit_Results\": \"Early Exit Results\",\n",
    "    \"Middle_exit_Results\": \"Middle Exit Results\",\n",
    "}\n",
    "\n",
    "logger.info(\"=== Grouped Categories Heatmap Analysis ===\")\n",
    "\n",
    "# List to collect all grouped category statistics for CSV export\n",
    "grouped_category_stats = []\n",
    "\n",
    "for fainder_mode in df_latest[\"fainder_mode\"].unique():\n",
    "    logger.info(f\"Analyzing fainder mode: {fainder_mode}\")\n",
    "    \n",
    "    for group_name, categories in groups_of_categories.items():\n",
    "        # Filter for relevant categories and fainder_mode\n",
    "        df_group = df_latest[\n",
    "            (df_latest[\"category\"].isin(categories))\n",
    "            & (df_latest[\"fainder_mode\"] == fainder_mode)\n",
    "        ]\n",
    "        if df_group.empty:\n",
    "            logger.info(f\"  Group {group_name}: No data available, skipping\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"  Group {group_name}: {len(df_group)} records\")\n",
    "\n",
    "        # Create a combined label for scenario and workers\n",
    "        df_group[\"scenario_workers\"] = df_group.apply(\n",
    "            lambda row: f\"{row['scenario']} - {row['fainder_max_workers']}\", axis=1\n",
    "        )\n",
    "\n",
    "        # Pivot table: rows=scenario-workers, columns=category, values=mean execution_time\n",
    "        pivot = df_group.pivot_table(\n",
    "            index=\"scenario_workers\",\n",
    "            columns=\"category\",\n",
    "            values=\"execution_time\",\n",
    "            aggfunc=\"mean\",\n",
    "        )\n",
    "\n",
    "        # Sort the index by workers (as integer) first, then by scenario (using ordered_scenarios)\n",
    "        def scenario_worker_sort_key(x):\n",
    "            scenario, worker = x.split(\" - \")\n",
    "            scenario_idx = (\n",
    "                ordered_scenarios.index(scenario)\n",
    "                if scenario in ordered_scenarios\n",
    "                else len(ordered_scenarios)\n",
    "            )\n",
    "            return (int(worker), scenario_idx)\n",
    "\n",
    "        pivot = pivot.reindex(sorted(pivot.index, key=scenario_worker_sort_key), axis=0)\n",
    "        pivot = pivot.rename(columns=category_short_names)\n",
    "\n",
    "        # Save pivot table to CSV\n",
    "        pivot_csv_file = csv_dir / f\"grouped_heatmap_{group_name}_{fainder_mode}_{timestamp}.csv\"\n",
    "        pivot.to_csv(pivot_csv_file)\n",
    "        logger.info(f\"    Saved pivot table to: {pivot_csv_file}\")\n",
    "\n",
    "        # Log pivot table statistics and collect for summary\n",
    "        logger.info(f\"    Pivot table shape: {pivot.shape}\")\n",
    "        pivot_values = pivot.values[~np.isnan(pivot.values)]\n",
    "        if len(pivot_values) > 0:\n",
    "            group_overall_stats = {\n",
    "                'fainder_mode': fainder_mode,\n",
    "                'group_name': group_name,\n",
    "                'categories_included': ', '.join(categories),\n",
    "                'shape_rows': pivot.shape[0],\n",
    "                'shape_cols': pivot.shape[1],\n",
    "                'mean': np.mean(pivot_values),\n",
    "                'min': np.min(pivot_values),\n",
    "                'max': np.max(pivot_values),\n",
    "                'std': np.std(pivot_values),\n",
    "                'valid_data_points': len(pivot_values),\n",
    "                'total_data_points': pivot.size\n",
    "            }\n",
    "            grouped_category_stats.append(group_overall_stats)\n",
    "            \n",
    "            logger.info(f\"    Overall stats - Mean: {group_overall_stats['mean']:.4f}s, \"\n",
    "                       f\"Min: {group_overall_stats['min']:.4f}s, \"\n",
    "                       f\"Max: {group_overall_stats['max']:.4f}s\")\n",
    "            \n",
    "            # Log per-category statistics\n",
    "            for col in pivot.columns:\n",
    "                col_values = pivot[col].dropna()\n",
    "                if len(col_values) > 0:\n",
    "                    logger.info(f\"      {col}: mean={col_values.mean():.4f}s, \"\n",
    "                               f\"min={col_values.min():.4f}s, max={col_values.max():.4f}s\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"label\": \"Execution Time (s)\"},\n",
    "        )\n",
    "        #plt.title(\n",
    "        #    f\"Execution Time Heatmap - {group_name.replace('_', ' ')} - {fainder_mode}\"\n",
    "        #)\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Scenario - fainder_max_workers\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"figures/analysis/heatmap_{group_name}_{fainder_mode}.png\"\n",
    "        plt.savefig(\n",
    "            filename,\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )\n",
    "        logger.info(f\"    Saved heatmap: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "# Save grouped category statistics to CSV\n",
    "if grouped_category_stats:\n",
    "    grouped_category_stats_df = pd.DataFrame(grouped_category_stats)\n",
    "    grouped_category_stats_file = csv_dir / f\"grouped_category_stats_{timestamp}.csv\"\n",
    "    grouped_category_stats_df.to_csv(grouped_category_stats_file, index=False)\n",
    "    logger.info(f\"Saved grouped category statistics to: {grouped_category_stats_file}\")\n",
    "\n",
    "logger.info(\"=== Analysis Complete ===\")\n",
    "logger.info(f\"All results logged to: {log_file}\")\n",
    "logger.info(f\"All CSV exports saved to: {csv_dir}\")\n",
    "\n",
    "# Create a summary of all exported files\n",
    "export_summary = []\n",
    "for csv_file in csv_dir.glob(f\"*_{timestamp}.csv\"):\n",
    "    file_size = csv_file.stat().st_size\n",
    "    export_summary.append({\n",
    "        'filename': csv_file.name,\n",
    "        'file_path': str(csv_file),\n",
    "        'size_bytes': file_size,\n",
    "        'size_mb': file_size / (1024 * 1024)\n",
    "    })\n",
    "\n",
    "if export_summary:\n",
    "    export_summary_df = pd.DataFrame(export_summary)\n",
    "    export_summary_file = csv_dir / f\"export_summary_{timestamp}.csv\"\n",
    "    export_summary_df.to_csv(export_summary_file, index=False)\n",
    "    logger.info(f\"Created export summary: {export_summary_file}\")\n",
    "    \n",
    "    total_size_mb = export_summary_df['size_mb'].sum()\n",
    "    logger.info(f\"Total exported data: {len(export_summary)} files, {total_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
