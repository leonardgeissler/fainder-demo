{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d970bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging and CSV export directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = Path(\"figures/optimizer/logs\")\n",
    "csv_dir = Path(\"figures/optimizer/csv_exports\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = log_dir / f\"optimizer_analysis_{timestamp}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Starting optimizer performance analysis - Log file: {log_file}\")\n",
    "logger.info(f\"CSV exports will be saved to: {csv_dir}\")\n",
    "\n",
    "# Get list of all CSV files in the directory\n",
    "list_of_commits = list(Path(\"../../logs/performance/analysis\").glob(\"*/\"))\n",
    "list_of_commits.sort(key=os.path.getmtime, reverse=True)\n",
    "list_of_commits = list_of_commits[:1]\n",
    "\n",
    "# Initialize an empty DataFrame to store combined results\n",
    "df_latest = pd.DataFrame()\n",
    "\n",
    "list_of_files = list(Path(list_of_commits[0] / \"all\").glob(\"*.csv\"))\n",
    "logger.info(f\"Processing {len(list_of_files)} CSV files from {list_of_commits[0]}\")\n",
    "\n",
    "# For each file in this commit\n",
    "for file_path in list_of_files:\n",
    "    # Read the CSV file\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "    # Append to the main DataFrame\n",
    "    df_latest = pd.concat([df_latest, temp_df], ignore_index=True)\n",
    "\n",
    "# Adjust num_workers: subtract 1 if not 0\n",
    "df_latest[\"fainder_max_workers\"] = df_latest[\"fainder_max_workers\"].apply(\n",
    "    lambda x: x - 1 if x != 0 else x\n",
    ")\n",
    "\n",
    "# Strip whitespace and normalize the category column\n",
    "df_latest[\"category\"] = df_latest[\"category\"].str.strip()\n",
    "\n",
    "logger.info(f\"Loaded {len(df_latest)} total records\")\n",
    "logger.info(f\"Unique categories: {list(df_latest['category'].unique())}\")\n",
    "logger.info(f\"Unique fainder modes: {list(df_latest['fainder_mode'].unique())}\")\n",
    "\n",
    "# Check optimizer columns\n",
    "optimizer_cols = ['optimizer_cost_sorting', 'optimizer_keyword_merging', 'optimizer_split_up_junctions']\n",
    "for col in optimizer_cols:\n",
    "    if col in df_latest.columns:\n",
    "        unique_vals = df_latest[col].unique()\n",
    "        logger.info(f\"{col}: {list(unique_vals)}\")\n",
    "    else:\n",
    "        logger.warning(f\"Column {col} not found in data\")\n",
    "\n",
    "# Save processed data to CSV\n",
    "processed_data_file = csv_dir / f\"processed_data_{timestamp}.csv\"\n",
    "df_latest.to_csv(processed_data_file, index=False)\n",
    "logger.info(f\"Saved processed data to: {processed_data_file}\")\n",
    "\n",
    "os.makedirs(\"figures/optimizer\", exist_ok=True)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df_latest.shape}\")\n",
    "print(\"Optimizer columns available:\")\n",
    "for col in optimizer_cols:\n",
    "    if col in df_latest.columns:\n",
    "        print(f\"  {col}: {df_latest[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# KEYWORD MERGING OPTIMIZATION ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "logger.info(\"=== Keyword Merging Optimization Analysis ===\")\n",
    "\n",
    "# Filter for categories that involve keywords and multiple elements\n",
    "keyword_categories = [\n",
    "    \"base_keyword_queries_with_multiple_elements\",\n",
    "]\n",
    "\n",
    "# Check if optimizer_keyword_merging column exists\n",
    "if 'optimizer_keyword_merging' not in df_latest.columns:\n",
    "    logger.warning(\"optimizer_keyword_merging column not found in data - skipping keyword merging analysis\")\n",
    "else:\n",
    "    keyword_merging_stats = []\n",
    "    \n",
    "    # Filter data for keyword-related categories\n",
    "    df_keywords = df_latest[df_latest[\"category\"].isin(keyword_categories)]\n",
    "    logger.info(f\"Found {len(df_keywords)} records for keyword-related categories\")\n",
    "    \n",
    "    if not df_keywords.empty:\n",
    "        # Group by optimizer setting, fainder_mode, and scenario\n",
    "        for category in keyword_categories:\n",
    "            df_cat = df_keywords[df_keywords[\"category\"] == category]\n",
    "            if df_cat.empty:\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Analyzing category: {category} ({len(df_cat)} records)\")\n",
    "            \n",
    "            # Create comparison plot for each fainder_mode\n",
    "            for fainder_mode in df_cat[\"fainder_mode\"].unique():\n",
    "                df_mode = df_cat[df_cat[\"fainder_mode\"] == fainder_mode]\n",
    "                if df_mode.empty:\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"  Fainder mode: {fainder_mode} ({len(df_mode)} records)\")\n",
    "                \n",
    "                # Group by optimizer setting and scenario\n",
    "                comparison_data = []\n",
    "                \n",
    "                for kw_merging in df_mode[\"optimizer_keyword_merging\"].unique():\n",
    "                    df_opt = df_mode[df_mode[\"optimizer_keyword_merging\"] == kw_merging]\n",
    "                    \n",
    "                    for scenario in df_opt[\"scenario\"].unique():\n",
    "                        df_scenario = df_opt[df_opt[\"scenario\"] == scenario]\n",
    "                        \n",
    "                        if not df_scenario.empty:\n",
    "                            mean_time = df_scenario[\"execution_time\"].mean()\n",
    "                            std_time = df_scenario[\"execution_time\"].std()\n",
    "                            min_time = df_scenario[\"execution_time\"].min()\n",
    "                            max_time = df_scenario[\"execution_time\"].max()\n",
    "                            count = len(df_scenario)\n",
    "                            \n",
    "                            comparison_data.append({\n",
    "                                'category': category,\n",
    "                                'fainder_mode': fainder_mode,\n",
    "                                'optimizer_keyword_merging': kw_merging,\n",
    "                                'scenario': scenario,\n",
    "                                'mean_execution_time': mean_time,\n",
    "                                'std_execution_time': std_time,\n",
    "                                'min_execution_time': min_time,\n",
    "                                'max_execution_time': max_time,\n",
    "                                'count': count\n",
    "                            })\n",
    "                            \n",
    "                            keyword_merging_stats.append({\n",
    "                                'category': category,\n",
    "                                'fainder_mode': fainder_mode,\n",
    "                                'optimizer_keyword_merging': kw_merging,\n",
    "                                'scenario': scenario,\n",
    "                                'mean': mean_time,\n",
    "                                'std': std_time,\n",
    "                                'min': min_time,\n",
    "                                'max': max_time,\n",
    "                                'count': count\n",
    "                            })\n",
    "                \n",
    "                if comparison_data:\n",
    "                    # Create comparison plot\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    \n",
    "                    comparison_df = pd.DataFrame(comparison_data)\n",
    "                    \n",
    "                    # Create grouped bar plot\n",
    "                    scenarios = comparison_df[\"scenario\"].unique()\n",
    "                    x = np.arange(len(scenarios))\n",
    "                    width = 0.35\n",
    "                    \n",
    "                    opt_on = comparison_df[comparison_df[\"optimizer_keyword_merging\"] == True]\n",
    "                    opt_off = comparison_df[comparison_df[\"optimizer_keyword_merging\"] == False]\n",
    "                    \n",
    "                    if not opt_on.empty and not opt_off.empty:\n",
    "                        # Align data by scenario\n",
    "                        on_means = [opt_on[opt_on[\"scenario\"] == s][\"mean_execution_time\"].iloc[0] \n",
    "                                  if not opt_on[opt_on[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        off_means = [opt_off[opt_off[\"scenario\"] == s][\"mean_execution_time\"].iloc[0] \n",
    "                                   if not opt_off[opt_off[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        \n",
    "                        on_stds = [opt_on[opt_on[\"scenario\"] == s][\"std_execution_time\"].iloc[0] \n",
    "                                 if not opt_on[opt_on[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        off_stds = [opt_off[opt_off[\"scenario\"] == s][\"std_execution_time\"].iloc[0] \n",
    "                                  if not opt_off[opt_off[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        \n",
    "                        plt.bar(x - width/2, off_means, width, label='Keyword Merging OFF', \n",
    "                               alpha=0.8, yerr=off_stds, capsize=5)\n",
    "                        plt.bar(x + width/2, on_means, width, label='Keyword Merging ON', \n",
    "                               alpha=0.8, yerr=on_stds, capsize=5)\n",
    "                        \n",
    "                        plt.xlabel('Scenario')\n",
    "                        plt.ylabel('Execution Time (s)')\n",
    "                        plt.title(f'Keyword Merging Impact - {category} - {fainder_mode}')\n",
    "                        plt.xticks(x, scenarios, rotation=45, ha='right')\n",
    "                        plt.legend()\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        filename = f\"figures/optimizer/keyword_merging_{category}_{fainder_mode}.png\"\n",
    "                        plt.savefig(filename)\n",
    "                        logger.info(f\"    Saved plot: {filename}\")\n",
    "                        plt.show()\n",
    "                        \n",
    "                        # Log performance comparison\n",
    "                        for i, scenario in enumerate(scenarios):\n",
    "                            if on_means[i] > 0 and off_means[i] > 0:\n",
    "                                improvement = ((off_means[i] - on_means[i]) / off_means[i]) * 100\n",
    "                                logger.info(f\"    {scenario}: {improvement:.2f}% improvement when ON\")\n",
    "    \n",
    "    # Save keyword merging statistics\n",
    "    if keyword_merging_stats:\n",
    "        kw_stats_df = pd.DataFrame(keyword_merging_stats)\n",
    "        kw_stats_file = csv_dir / f\"keyword_merging_stats_{timestamp}.csv\"\n",
    "        kw_stats_df.to_csv(kw_stats_file, index=False)\n",
    "        logger.info(f\"Saved keyword merging statistics to: {kw_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# COST SORTING OPTIMIZATION ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "logger.info(\"=== Cost Sorting Optimization Analysis ===\")\n",
    "\n",
    "# Filter for categories that involve multiple percentile combinations\n",
    "cost_sorting_categories = [\n",
    "    \"multiple_percentile_combinations\",\n",
    "]\n",
    "\n",
    "# Check if optimizer_cost_sorting column exists\n",
    "if 'optimizer_cost_sorting' not in df_latest.columns:\n",
    "    logger.warning(\"optimizer_cost_sorting column not found in data - skipping cost sorting analysis\")\n",
    "else:\n",
    "    cost_sorting_stats = []\n",
    "    \n",
    "    # Filter data for percentile-related categories\n",
    "    df_cost_sorting = df_latest[df_latest[\"category\"].isin(cost_sorting_categories)]\n",
    "    logger.info(f\"Found {len(df_cost_sorting)} records for cost sorting-related categories\")\n",
    "    \n",
    "    if not df_cost_sorting.empty:\n",
    "        # Create comprehensive comparison\n",
    "        for category in cost_sorting_categories:\n",
    "            df_cat = df_cost_sorting[df_cost_sorting[\"category\"] == category]\n",
    "            if df_cat.empty:\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Analyzing category: {category} ({len(df_cat)} records)\")\n",
    "            \n",
    "            # Create comparison plot for each fainder_mode\n",
    "            for fainder_mode in df_cat[\"fainder_mode\"].unique():\n",
    "                df_mode = df_cat[df_cat[\"fainder_mode\"] == fainder_mode]\n",
    "                if df_mode.empty:\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"  Fainder mode: {fainder_mode} ({len(df_mode)} records)\")\n",
    "                \n",
    "                # Group by optimizer setting, scenario, and workers\n",
    "                comparison_data = []\n",
    "                \n",
    "                for cost_sorting in df_mode[\"optimizer_cost_sorting\"].unique():\n",
    "                    df_opt = df_mode[df_mode[\"optimizer_cost_sorting\"] == cost_sorting]\n",
    "                    \n",
    "                    for scenario in df_opt[\"scenario\"].unique():\n",
    "                        df_scenario = df_opt[df_opt[\"scenario\"] == scenario]\n",
    "                        \n",
    "                        for workers in df_scenario[\"fainder_max_workers\"].unique():\n",
    "                            df_workers = df_scenario[df_scenario[\"fainder_max_workers\"] == workers]\n",
    "                            \n",
    "                            if not df_workers.empty:\n",
    "                                mean_time = df_workers[\"execution_time\"].mean()\n",
    "                                std_time = df_workers[\"execution_time\"].std()\n",
    "                                min_time = df_workers[\"execution_time\"].min()\n",
    "                                max_time = df_workers[\"execution_time\"].max()\n",
    "                                count = len(df_workers)\n",
    "                                \n",
    "                                comparison_data.append({\n",
    "                                    'category': category,\n",
    "                                    'fainder_mode': fainder_mode,\n",
    "                                    'optimizer_cost_sorting': cost_sorting,\n",
    "                                    'scenario': scenario,\n",
    "                                    'workers': workers,\n",
    "                                    'mean_execution_time': mean_time,\n",
    "                                    'std_execution_time': std_time,\n",
    "                                    'min_execution_time': min_time,\n",
    "                                    'max_execution_time': max_time,\n",
    "                                    'count': count\n",
    "                                })\n",
    "                                \n",
    "                                cost_sorting_stats.append({\n",
    "                                    'category': category,\n",
    "                                    'fainder_mode': fainder_mode,\n",
    "                                    'optimizer_cost_sorting': cost_sorting,\n",
    "                                    'scenario': scenario,\n",
    "                                    'workers': workers,\n",
    "                                    'mean': mean_time,\n",
    "                                    'std': std_time,\n",
    "                                    'min': min_time,\n",
    "                                    'max': max_time,\n",
    "                                    'count': count\n",
    "                                })\n",
    "                \n",
    "                if comparison_data:\n",
    "                    comparison_df = pd.DataFrame(comparison_data)\n",
    "                    \n",
    "                    # Create pivot table for heatmap\n",
    "                    pivot_on = comparison_df[comparison_df[\"optimizer_cost_sorting\"] == True].pivot_table(\n",
    "                        index=[\"scenario\", \"workers\"], \n",
    "                        values=\"mean_execution_time\", \n",
    "                        aggfunc=\"mean\"\n",
    "                    )\n",
    "                    \n",
    "                    pivot_off = comparison_df[comparison_df[\"optimizer_cost_sorting\"] == False].pivot_table(\n",
    "                        index=[\"scenario\", \"workers\"], \n",
    "                        values=\"mean_execution_time\", \n",
    "                        aggfunc=\"mean\"\n",
    "                    )\n",
    "                    \n",
    "                    # Create side-by-side comparison plot\n",
    "                    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                    \n",
    "                    # Heatmap for Cost Sorting OFF\n",
    "                    if not pivot_off.empty:\n",
    "                        sns.heatmap(pivot_off.unstack(fill_value=np.nan), \n",
    "                                   annot=True, fmt='.3f', cmap='viridis', ax=ax1,\n",
    "                                   cbar_kws={'label': 'Execution Time (s)'})\n",
    "                        ax1.set_title(f'Cost Sorting OFF\\n{category} - {fainder_mode}')\n",
    "                        ax1.set_xlabel('Workers')\n",
    "                        ax1.set_ylabel('Scenario')\n",
    "                    \n",
    "                    # Heatmap for Cost Sorting ON\n",
    "                    if not pivot_on.empty:\n",
    "                        sns.heatmap(pivot_on.unstack(fill_value=np.nan), \n",
    "                                   annot=True, fmt='.3f', cmap='viridis', ax=ax2,\n",
    "                                   cbar_kws={'label': 'Execution Time (s)'})\n",
    "                        ax2.set_title(f'Cost Sorting ON\\n{category} - {fainder_mode}')\n",
    "                        ax2.set_xlabel('Workers')\n",
    "                        ax2.set_ylabel('Scenario')\n",
    "                    \n",
    "                    # Improvement heatmap\n",
    "                    if not pivot_off.empty and not pivot_on.empty:\n",
    "                        # Calculate improvement percentage\n",
    "                        improvement = ((pivot_off - pivot_on) / pivot_off * 100).fillna(0)\n",
    "                        \n",
    "                        sns.heatmap(improvement.unstack(fill_value=np.nan), \n",
    "                                   annot=True, fmt='.1f', cmap='RdYlGn', center=0, ax=ax3,\n",
    "                                   cbar_kws={'label': 'Improvement (%)'})\n",
    "                        ax3.set_title(f'Performance Improvement\\n{category} - {fainder_mode}')\n",
    "                        ax3.set_xlabel('Workers')\n",
    "                        ax3.set_ylabel('Scenario')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    filename = f\"figures/optimizer/cost_sorting_{category}_{fainder_mode}.png\"\n",
    "                    plt.savefig(filename)\n",
    "                    logger.info(f\"    Saved plot: {filename}\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Log summary statistics\n",
    "                    if not pivot_off.empty and not pivot_on.empty:\n",
    "                        overall_improvement = ((pivot_off.mean() - pivot_on.mean()) / pivot_off.mean()) * 100\n",
    "                        logger.info(f\"    Overall improvement with cost sorting: {overall_improvement:.2f}%\")\n",
    "    \n",
    "    # Save cost sorting statistics\n",
    "    if cost_sorting_stats:\n",
    "        cost_stats_df = pd.DataFrame(cost_sorting_stats)\n",
    "        cost_stats_file = csv_dir / f\"cost_sorting_stats_{timestamp}.csv\"\n",
    "        cost_stats_df.to_csv(cost_stats_file, index=False)\n",
    "        logger.info(f\"Saved cost sorting statistics to: {cost_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SPLIT-UP JUNCTIONS OPTIMIZATION ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "logger.info(\"=== Split-up Junctions Optimization Analysis ===\")\n",
    "\n",
    "# Filter for categories that involve multiple percentile combinations (where junctions matter)\n",
    "junction_categories = [\n",
    "    \"multiple_percentile_combinations\",\n",
    "]\n",
    "\n",
    "# Check if optimizer_split_up_junctions column exists\n",
    "if 'optimizer_split_up_junctions' not in df_latest.columns:\n",
    "    logger.warning(\"optimizer_split_up_junctions column not found in data - skipping split-up junctions analysis\")\n",
    "else:\n",
    "    junction_stats = []\n",
    "    \n",
    "    # Filter data for junction-related categories\n",
    "    df_junctions = df_latest[df_latest[\"category\"].isin(junction_categories)]\n",
    "    logger.info(f\"Found {len(df_junctions)} records for junction-related categories\")\n",
    "    \n",
    "    if not df_junctions.empty:\n",
    "        # Create comprehensive comparison\n",
    "        for category in junction_categories:\n",
    "            df_cat = df_junctions[df_junctions[\"category\"] == category]\n",
    "            if df_cat.empty:\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Analyzing category: {category} ({len(df_cat)} records)\")\n",
    "            \n",
    "            # Create comparison plot for each fainder_mode\n",
    "            for fainder_mode in df_cat[\"fainder_mode\"].unique():\n",
    "                df_mode = df_cat[df_cat[\"fainder_mode\"] == fainder_mode]\n",
    "                if df_mode.empty:\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"  Fainder mode: {fainder_mode} ({len(df_mode)} records)\")\n",
    "                \n",
    "                # Group by optimizer setting and scenario\n",
    "                comparison_data = []\n",
    "                \n",
    "                for split_junctions in df_mode[\"optimizer_split_up_junctions\"].unique():\n",
    "                    df_opt = df_mode[df_mode[\"optimizer_split_up_junctions\"] == split_junctions]\n",
    "                    \n",
    "                    for scenario in df_opt[\"scenario\"].unique():\n",
    "                        df_scenario = df_opt[df_opt[\"scenario\"] == scenario]\n",
    "                        \n",
    "                        if not df_scenario.empty:\n",
    "                            mean_time = df_scenario[\"execution_time\"].mean()\n",
    "                            std_time = df_scenario[\"execution_time\"].std()\n",
    "                            min_time = df_scenario[\"execution_time\"].min()\n",
    "                            max_time = df_scenario[\"execution_time\"].max()\n",
    "                            count = len(df_scenario)\n",
    "                            median_time = df_scenario[\"execution_time\"].median()\n",
    "                            \n",
    "                            comparison_data.append({\n",
    "                                'category': category,\n",
    "                                'fainder_mode': fainder_mode,\n",
    "                                'optimizer_split_up_junctions': split_junctions,\n",
    "                                'scenario': scenario,\n",
    "                                'mean_execution_time': mean_time,\n",
    "                                'median_execution_time': median_time,\n",
    "                                'std_execution_time': std_time,\n",
    "                                'min_execution_time': min_time,\n",
    "                                'max_execution_time': max_time,\n",
    "                                'count': count\n",
    "                            })\n",
    "                            \n",
    "                            junction_stats.append({\n",
    "                                'category': category,\n",
    "                                'fainder_mode': fainder_mode,\n",
    "                                'optimizer_split_up_junctions': split_junctions,\n",
    "                                'scenario': scenario,\n",
    "                                'mean': mean_time,\n",
    "                                'median': median_time,\n",
    "                                'std': std_time,\n",
    "                                'min': min_time,\n",
    "                                'max': max_time,\n",
    "                                'count': count\n",
    "                            })\n",
    "                \n",
    "                if comparison_data:\n",
    "                    comparison_df = pd.DataFrame(comparison_data)\n",
    "                    \n",
    "                    # Create comparison plots\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    # Bar plot comparison\n",
    "                    scenarios = comparison_df[\"scenario\"].unique()\n",
    "                    x = np.arange(len(scenarios))\n",
    "                    width = 0.35\n",
    "                    \n",
    "                    opt_on = comparison_df[comparison_df[\"optimizer_split_up_junctions\"] == True]\n",
    "                    opt_off = comparison_df[comparison_df[\"optimizer_split_up_junctions\"] == False]\n",
    "                    \n",
    "                    if not opt_on.empty and not opt_off.empty:\n",
    "                        # Align data by scenario\n",
    "                        on_means = [opt_on[opt_on[\"scenario\"] == s][\"mean_execution_time\"].iloc[0] \n",
    "                                  if not opt_on[opt_on[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        off_means = [opt_off[opt_off[\"scenario\"] == s][\"mean_execution_time\"].iloc[0] \n",
    "                                   if not opt_off[opt_off[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        \n",
    "                        on_stds = [opt_on[opt_on[\"scenario\"] == s][\"std_execution_time\"].iloc[0] \n",
    "                                 if not opt_on[opt_on[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        off_stds = [opt_off[opt_off[\"scenario\"] == s][\"std_execution_time\"].iloc[0] \n",
    "                                  if not opt_off[opt_off[\"scenario\"] == s].empty else 0 for s in scenarios]\n",
    "                        \n",
    "                        ax1.bar(x - width/2, off_means, width, label='Split Junctions OFF', \n",
    "                               alpha=0.8, yerr=off_stds, capsize=5, color='lightcoral')\n",
    "                        ax1.bar(x + width/2, on_means, width, label='Split Junctions ON', \n",
    "                               alpha=0.8, yerr=on_stds, capsize=5, color='lightblue')\n",
    "                        \n",
    "                        ax1.set_xlabel('Scenario')\n",
    "                        ax1.set_ylabel('Execution Time (s)')\n",
    "                        ax1.set_title(f'Split Junctions Impact - {category} - {fainder_mode}')\n",
    "                        ax1.set_xticks(x)\n",
    "                        ax1.set_xticklabels(scenarios, rotation=45, ha='right')\n",
    "                        ax1.legend()\n",
    "                        \n",
    "                        # Improvement percentages\n",
    "                        improvements = []\n",
    "                        scenario_labels = []\n",
    "                        for i, scenario in enumerate(scenarios):\n",
    "                            if on_means[i] > 0 and off_means[i] > 0:\n",
    "                                improvement = ((off_means[i] - on_means[i]) / off_means[i]) * 100\n",
    "                                improvements.append(improvement)\n",
    "                                scenario_labels.append(scenario)\n",
    "                                logger.info(f\"    {scenario}: {improvement:.2f}% improvement when ON\")\n",
    "                        \n",
    "                        if improvements:\n",
    "                            colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "                            ax2.bar(range(len(improvements)), improvements, color=colors, alpha=0.7)\n",
    "                            ax2.set_xlabel('Scenario')\n",
    "                            ax2.set_ylabel('Performance Improvement (%)')\n",
    "                            ax2.set_title(f'Split Junctions Improvement - {category} - {fainder_mode}')\n",
    "                            ax2.set_xticks(range(len(scenario_labels)))\n",
    "                            ax2.set_xticklabels(scenario_labels, rotation=45, ha='right')\n",
    "                            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "                            ax2.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        filename = f\"figures/optimizer/split_junctions_{category}_{fainder_mode}.png\"\n",
    "                        plt.savefig(filename)\n",
    "                        logger.info(f\"    Saved plot: {filename}\")\n",
    "                        plt.show()\n",
    "    \n",
    "    # Save split-up junctions statistics\n",
    "    if junction_stats:\n",
    "        junction_stats_df = pd.DataFrame(junction_stats)\n",
    "        junction_stats_file = csv_dir / f\"split_junctions_stats_{timestamp}.csv\"\n",
    "        junction_stats_df.to_csv(junction_stats_file, index=False)\n",
    "        logger.info(f\"Saved split-up junctions statistics to: {junction_stats_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
